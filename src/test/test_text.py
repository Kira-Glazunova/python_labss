import pytest
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.append("C:/Users/kira_/OneDrive/–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª/python_labss/")

from src.lib.text import normalize, tokenize, count_freq, top_n


class TestNormalize:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ normalize()"""

    @pytest.mark.parametrize(
        "input_text, expected",
        [
            ("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t", "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"),
            ("—ë–∂–∏–∫, –Å–ª–∫–∞", "–µ–∂–∏–∫, –µ–ª–∫–∞"),
            ("Hello\r\nWorld", "hello world"),
            ("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ", "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"),
            ("", ""),
            ("   ", ""),
            ("–¢–µ–∫—Å—Ç\n—Å\n–ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏", "—Ç–µ–∫—Å—Ç —Å –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏"),
            ("–†–∞–∑\t\t–¥–≤–∞\t—Ç—Ä–∏", "—Ä–∞–∑ –¥–≤–∞ —Ç—Ä–∏"),
            ("–í–µ—Ä—Ö–Ω–∏–π –†–ï–ì–ò–°–¢–†", "–≤–µ—Ä—Ö–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä"),
            ("—Å–º–µ—Å—å –Å –∏ –ï", "—Å–º–µ—Å—å –µ –∏ –µ"),
        ],
    )
    def test_normalize_basic(self, input_text, expected):
        """–ë–∞–∑–æ–≤—ã–µ —Ç–µ—Å—Ç—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
        assert normalize(input_text) == expected

    def test_normalize_casefold_false(self):
        """–¢–µ—Å—Ç —Å –æ—Ç–∫–ª—é—á–µ–Ω–Ω—ã–º casefold"""
        text = "–ü—Ä–ò–≤–ï—Ç –ú–∏–†"
        result = normalize(text, casefold=False)
        assert result == "–ü—Ä–ò–≤–ï—Ç –ú–∏–†"

    def test_normalize_yo2e_false(self):
        """–¢–µ—Å—Ç —Å –æ—Ç–∫–ª—é—á–µ–Ω–Ω–æ–π –∑–∞–º–µ–Ω–æ–π —ë –Ω–∞ –µ"""
        text = "—ë–∂–∏–∫ –Å–ª–∫–∞"
        result = normalize(text, yo2e=False)
        assert result == "—ë–∂–∏–∫ —ë–ª–∫–∞"


class TestTokenize:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ tokenize()"""

    @pytest.mark.parametrize(
        "input_text, expected",
        [
            ("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä", ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]),
            ("hello,world!!!", ["hello", "world"]),
            ("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ", ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]),
            ("2025 –≥–æ–¥", ["2025", "–≥–æ–¥"]),
            ("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ", ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]),
            ("", []),
            ("   ", []),
            ("!!! @#$ %^&*", []),
            ("—Å–ª–æ–≤–æ-—Å-–¥–µ—Ñ–∏—Å–æ–º –∏ –µ—â–µ", ["—Å–ª–æ–≤–æ-—Å-–¥–µ—Ñ–∏—Å–æ–º", "–∏", "–µ—â–µ"]),
            ("–º–Ω–æ–≥–æ     –ø—Ä–æ–±–µ–ª–æ–≤", ["–º–Ω–æ–≥–æ", "–ø—Ä–æ–±–µ–ª–æ–≤"]),
        ],
    )
    def test_tokenize_basic(self, input_text, expected):
        """–ë–∞–∑–æ–≤—ã–µ —Ç–µ—Å—Ç—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"""
        assert tokenize(input_text) == expected

    def test_tokenize_with_normalized_text(self):
        """–¢–µ—Å—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏"""
        text = "–ü—Ä–ò–≤–ï—Ç, –ú–∏–†! 2025-–π –≥–æ–¥."
        normalized = normalize(text)
        tokens = tokenize(normalized)
        assert tokens == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä", "2025", "–π", "–≥–æ–¥"]


class TestCountFreq:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ count_freq()"""

    def test_count_freq_basic(self):
        """–ë–∞–∑–æ–≤—ã–π —Ç–µ—Å—Ç –ø–æ–¥—Å—á–µ—Ç–∞ —á–∞—Å—Ç–æ—Ç"""
        tokens = ["a", "b", "a", "c", "b", "a"]
        result = count_freq(tokens)
        expected = {"a": 3, "b": 2, "c": 1}
        assert result == expected

    def test_count_freq_empty(self):
        """–¢–µ—Å—Ç —Å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º"""
        result = count_freq([])
        assert result == {}

    def test_count_freq_single_token(self):
        """–¢–µ—Å—Ç —Å –æ–¥–Ω–∏–º —Ç–æ–∫–µ–Ω–æ–º"""
        result = count_freq(["—Å–ª–æ–≤–æ"])
        assert result == {"—Å–ª–æ–≤–æ": 1}

    def test_count_freq_case_sensitive(self):
        """–¢–µ—Å—Ç —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ —Ä–µ–≥–∏—Å—Ç—Ä—É"""
        tokens = ["Word", "word", "WORD"]
        result = count_freq(tokens)
        assert result == {"Word": 1, "word": 1, "WORD": 1}


class TestTopN:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ top_n()"""

    def test_top_n_basic(self):
        """–ë–∞–∑–æ–≤—ã–π —Ç–µ—Å—Ç top_n"""
        freq = {"a": 5, "b": 3, "c": 7, "d": 1, "e": 4}
        result = top_n(freq, n=3)
        expected = [("c", 7), ("a", 5), ("e", 4)]
        assert result == expected

    def test_top_n_tie_breaker(self):
        """–¢–µ—Å—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∏—á—å–∏—Ö –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É"""
        freq = {"banana": 3, "apple": 3, "cherry": 3, "date": 2}
        result = top_n(freq, n=3)
        # –ü—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —á–∞—Å—Ç–æ—Ç–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
        expected = [("apple", 3), ("banana", 3), ("cherry", 3)]
        assert result == expected

    def test_top_n_more_than_available(self):
        """–¢–µ—Å—Ç –∫–æ–≥–¥–∞ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –±–æ–ª—å—à–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —á–µ–º –µ—Å—Ç—å"""
        freq = {"a": 2, "b": 1}
        result = top_n(freq, n=10)
        assert result == [("a", 2), ("b", 1)]

    def test_top_n_empty_dict(self):
        """–¢–µ—Å—Ç —Å –ø—É—Å—Ç—ã–º —Å–ª–æ–≤–∞—Ä–µ–º"""
        result = top_n({}, n=5)
        assert result == []

    def test_top_n_n_zero(self):
        """–¢–µ—Å—Ç —Å n=0"""
        freq = {"a": 1, "b": 2}
        result = top_n(freq, n=0)
        assert result == []

    def test_top_n_negative_n(self):
        """–¢–µ—Å—Ç —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º n"""
        freq = {"a": 1, "b": 2}
        result = top_n(freq, n=-1)
        assert result == []

    @pytest.mark.parametrize(
        "n, expected",
        [
            (1, [("c", 3)]),
            (2, [("c", 3), ("a", 2)]),
            (3, [("c", 3), ("a", 2), ("b", 1)]),
        ],
    )
    def test_top_n_parametrized(self, n, expected):
        """–ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ—Å—Ç top_n"""
        freq = {"a": 2, "b": 1, "c": 3}
        result = top_n(freq, n=n)
        assert result == expected


class TestIntegration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π –≤–º–µ—Å—Ç–µ"""

    def test_full_pipeline(self):
        """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è -> —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è -> –ø–æ–¥—Å—á–µ—Ç -> —Ç–æ–ø"""
        text = "–ü—Ä–∏–≤–µ—Ç –º–∏—Ä! –ü—Ä–∏–≤–µ—Ç –≤—Å–µ–º. –ú–∏—Ä –ø—Ä–µ–∫—Ä–∞—Å–µ–Ω."

        normalized = normalize(text)
        assert normalized == "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä –ø—Ä–∏–≤–µ—Ç –≤—Å–µ–º –º–∏—Ä –ø—Ä–µ–∫—Ä–∞—Å–µ–Ω"

        tokens = tokenize(normalized)
        assert tokens == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä", "–ø—Ä–∏–≤–µ—Ç", "–≤—Å–µ–º", "–º–∏—Ä", "–ø—Ä–µ–∫—Ä–∞—Å–µ–Ω"]

        freq = count_freq(tokens)
        assert freq == {"–ø—Ä–∏–≤–µ—Ç": 2, "–º–∏—Ä": 2, "–≤—Å–µ–º": 1, "–ø—Ä–µ–∫—Ä–∞—Å–µ–Ω": 1}

        top = top_n(freq, n=2)
        # –ü—Ä–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —á–∞—Å—Ç–æ—Ç–µ "–º–∏—Ä" –∏–¥–µ—Ç –ø–µ—Ä–µ–¥ "–ø—Ä–∏–≤–µ—Ç" –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
        assert top == [("–º–∏—Ä", 2), ("–ø—Ä–∏–≤–µ—Ç", 2)]

    def test_empty_text_pipeline(self):
        """–¢–µ—Å—Ç –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å –ø—É—Å—Ç—ã–º —Ç–µ–∫—Å—Ç–æ–º"""
        text = ""

        normalized = normalize(text)
        assert normalized == ""

        tokens = tokenize(normalized)
        assert tokens == []

        freq = count_freq(tokens)
        assert freq == {}

        top = top_n(freq, n=5)
        assert top == []
